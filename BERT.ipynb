{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZLhDrSl89c8lzitdo9wra",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RickBerends/Domain-Specific-BERT-for-Depression-Detection/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H6X_ob5kEeF",
        "outputId": "05d499bb-07a8-407f-85a3-ecb558aef020"
      },
      "source": [
        "!pip install transformers==3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.0.45)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3) (20.9)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.1.95)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (8.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1Nkc68hj98v",
        "outputId": "93fc96cf-f673-47dc-fe12-c684cd6793fe"
      },
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYgGz8FAk1WA",
        "outputId": "791ce654-de3a-4131-d29f-590ced53a784"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "sys.path.insert(0, '/content/drive/Mijn Drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbyp2MqCk85w"
      },
      "source": [
        "os.chdir('/content/drive/My Drive/Splits')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFdYceaFk_fS"
      },
      "source": [
        "os.getcwd()\n",
        "\n",
        "#Load train, split = 0 for train. \n",
        "df = pd.read_csv(\"train_split.csv\", usecols=[0,1])\n",
        "df['split'] = 0\n",
        "df['Participant_ID'].astype(int)\n",
        "\n",
        "#Load test split, split = 2 for test.\n",
        "df1 = pd.read_csv(\"full_test_split copy.csv\", sep=';', usecols=[0,1])\n",
        "df1['split'] = 2\n",
        "df1['Participant_ID'].astype(int)\n",
        "df1.rename(columns = {'PHQ_Binary':'PHQ8_Binary'}, inplace = True)\n",
        "\n",
        "#Load dev split, split = 1 for dev.\n",
        "df2 = pd.read_csv(\"dev_split.csv\", usecols=[0,1])\n",
        "df2['split'] = 1\n",
        "df2['Participant_ID'].astype(int)\n",
        "\n",
        "#Get them together\n",
        "final_df = pd.concat([df,df1,df2])\n",
        "final_df.sort_values('Participant_ID', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdPRyUDY6c20",
        "outputId": "f05e0e9b-4b5b-47e7-e5e9-f89adddffb85"
      },
      "source": [
        "#Building data cleaning. \n",
        "\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text): \n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  text = text.lower()\n",
        "  text = re.sub(r\"<sync>\", \"\",text)\n",
        "  text = re.sub(r\"he's\", \"he is\", text)\n",
        "  text = re.sub(r\"there's\", \"there is\", text)\n",
        "  text = re.sub(r\"we're\", \"we are\", text)\n",
        "  text = re.sub(r\"That's\", \"That is\", text)\n",
        "  text = re.sub(r\"won't\", \"will not\", text)\n",
        "  text = re.sub(r\"they're\", \"they are\", text)\n",
        "  text = re.sub(r\"can't\", \"cannot\", text)\n",
        "  text = re.sub(r\"wasn't\", \"was not\", text)\n",
        "  text = re.sub(r\"don't\", \"do not\", text)\n",
        "  text = re.sub(r\"aren't\", \"are not\", text)\n",
        "  text = re.sub(r\"isn't\", \"is not\", text)\n",
        "  text = re.sub(r\"what's\", \"what is\", text)\n",
        "  text = re.sub(r\"haven't\", \"have not\", text)\n",
        "  text = re.sub(r\"hasn't\", \"has not\", text)\n",
        "  text = re.sub(r\"there's\", \"there is\", text)\n",
        "  text = re.sub(r\"he's\", \"he is\", text)\n",
        "  text = re.sub(r\"it's\", \"it is\", text)\n",
        "  text = re.sub(r\"you're\", \"you are\", text)\n",
        "  text = re.sub(r\"i'm\", \"i am\", text)\n",
        "  text = re.sub(r\"shouldn't\", \"should not\", text)\n",
        "  text = re.sub(r\"wouldn't\", \"would not\", text)\n",
        "  text = re.sub(r\"isn't\", \"is not\", text)\n",
        "  text = re.sub(r\"here's\", \"here is\", text)\n",
        "  text = re.sub(r\"you've\", \"you have\", text)\n",
        "  text = re.sub(r\"we're\", \"we are\", text)\n",
        "  text = re.sub(r\"what's\", \"what is\", text)\n",
        "  text = re.sub(r\"couldn't\", \"could not\", text)\n",
        "  text = re.sub(r\"we've\", \"we have\", text)\n",
        "  text = re.sub(r\"it's\", \"it is\", text)\n",
        "  text = re.sub(r\"doesn't\", \"does not\", text)\n",
        "  text = re.sub(r\"who's\", \"who is\", text)\n",
        "  text = re.sub(r\"y'all\", \"you all\", text)\n",
        "  text = re.sub(r\"would've\", \"would have\", text)\n",
        "  text = re.sub(r\"it'll\", \"it will\", text)\n",
        "  text = re.sub(r\"we'll\", \"we will\", text)\n",
        "  text = re.sub(r\"we've\", \"we have\", text)\n",
        "  text = re.sub(r\"he'll\", \"he will\", text)\n",
        "  text = re.sub(r\"y'all\", \"you all\", text)\n",
        "  text = re.sub(r\"weren't\", \"Were not\", text)\n",
        "  text = re.sub(r\"didn't\", \"did not\", text)\n",
        "  text = re.sub(r\"they'll\", \"they will\", text)\n",
        "  text = re.sub(r\"they'd\", \"they would\", text)\n",
        "  text = re.sub(r\"don't\", \"do not\", text)\n",
        "  text = re.sub(r\"they've\", \"they have\", text)\n",
        "  text = re.sub(r\"i'd\", \"I would\", text)\n",
        "  text = re.sub(r\"should've\", \"should have\", text)\n",
        "  text = re.sub(r\"where's\", \"where is\", text)\n",
        "  text = re.sub(r\"we'd\", \"we would\", text)\n",
        "  text = re.sub(r\"i'll\", \"i will\", text)\n",
        "  text = re.sub(r\"weren't\", \"were not\", text)\n",
        "  text = re.sub(r\"they're\", \"they are\", text)\n",
        "  text = re.sub(r\"let's\", \"let us\", text)\n",
        "  text = re.sub(r\"it's\", \"it is\", text)\n",
        "  text = re.sub(r\"can't\", \"cannot\", text)\n",
        "  text = re.sub(r\"don't\", \"do not\", text)\n",
        "  text = re.sub(r\"you're\", \"you are\", text)\n",
        "  text = re.sub(r\"i've\", \"i have\", text)\n",
        "  text = re.sub(r\"that's\", \"that is\", text)\n",
        "  text = re.sub(r\"i'll\", \"i will\", text)\n",
        "  text = re.sub(r\"doesn't\", \"does not\", text)\n",
        "  text = re.sub(r\"i'd\", \"i would\", text)\n",
        "  text = re.sub(r\"didn't\", \"did not\", text)\n",
        "  text = re.sub(r\"ain't\", \"am not\", text)\n",
        "  text = re.sub(r\"you'll\", \"you will\", text)\n",
        "  text = re.sub(r\"i've\", \"i have\", text)\n",
        "  text = re.sub(r\"Don't\", \"do not\", text)\n",
        "  text = re.sub(r\"i'll\", \"i will\", text)\n",
        "  text = re.sub(r\"i'd\", \"i would\", text)\n",
        "  text = re.sub(r\"let's\", \"Let us\", text)\n",
        "  text = re.sub(r\"you'd\", \"you would\", text)\n",
        "  text = re.sub(r\"it's\", \"it is\", text)\n",
        "  text = re.sub(r\"ain't\", \"am not\", text)\n",
        "  text = re.sub(r\"haven't\", \"have not\", text)\n",
        "  text = re.sub(r\"could've\", \"could have\", text)\n",
        "  text = re.sub(r\"youve\", \"you have\", text)  \n",
        "  text = re.sub(r\"[-()\\\"#/@;:<>.?,]\",\"\",text)\n",
        "  text = ' '.join(text.split())\n",
        "  text = text.split(' ')\n",
        "  text = [w for w in text if not w in stop_words]\n",
        "  text = [lemmatizer.lemmatize(w) for w in text]\n",
        "  text = ' '.join(text)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzdLX8C5lFRF"
      },
      "source": [
        "#Loading all the text from the individual transcripts into one dataframe. \n",
        "os.chdir('/content/drive/My Drive/DAICWOZ_transcripts')\n",
        "\n",
        "lst = []\n",
        "labels = []\n",
        "pd.set_option('max_colwidth', 800)\n",
        "\n",
        "for file in glob.glob(\"*.csv\"):\n",
        "    name = file\n",
        "    try: \n",
        "        df = pd.read_csv(name, usecols=[2,3], sep='\\t')\n",
        "        outcome = df['speaker'] == 'Participant' \n",
        "        new_str = ''\n",
        "    except ValueError: \n",
        "        print(name)\n",
        "        continue\n",
        "    for index, j in enumerate(outcome):\n",
        "        if j == False: \n",
        "            continue\n",
        "        else: \n",
        "            response = df['value'][index:index+1]\n",
        "            response = response.to_string(index=False)\n",
        "            new_str+= response\n",
        "    new_str = clean_text(new_str)\n",
        "    lst.append(new_str)\n",
        "    labels.append(name.split('_')[0])\n",
        "\n",
        "final_df['text'] = lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9G26-X1lId7"
      },
      "source": [
        "#Split the text, based on the length of the text. \n",
        "\n",
        "def get_split(text1): \n",
        "  l_total = []\n",
        "  l_partial = []\n",
        "\n",
        "  #Check whether or not the text is bigger than 150 words and should be split.\n",
        "  if len(text1.split())//20>0:\n",
        "    n = len(text1.split())//20\n",
        "  else: \n",
        "    n = 1\n",
        "  \n",
        "  #If that's the case, the first split is smaller than 200, the next split is [150:350], [300:550] etc. \n",
        "  for w in range(n): \n",
        "    if w == 0: \n",
        "      l_partial = text1.split()[:20]\n",
        "      l_total.append(\" \".join(l_partial))\n",
        "\n",
        "    else: \n",
        "      l_partial = text1.split()[w*20: w*20+ 20]\n",
        "      l_total.append(\" \".join(l_partial))\n",
        "  return l_total\n",
        "\n",
        "twohundred_lst = []\n",
        "\n",
        "for i in lst: \n",
        "  twohundred_lst.append(get_split(i))\n",
        "\n",
        "#Finally replace the \"text\" with text, split up in lists.\n",
        "final_df['text'] = twohundred_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cabjmnXbz_W4",
        "outputId": "74ceebc4-c2d5-492e-c13a-5038152e79f2"
      },
      "source": [
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Participant_ID</th>\n",
              "      <th>PHQ8_Binary</th>\n",
              "      <th>split</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>300</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>[yes i am doing well los angeles the greater los angeles area people diversity and various entertainment and activities um, fickle weather traffic and litter i have traveled domestically not internationally seeing other places and how people live and the, culture the last trip that i went to was san diego but uh from childhood or as an adult um, family vacations to um baja and rosarito beach yeah the reason why that memory comes to mind because um this, weekend is easter and we usually will go down to easter east easter spring break to rosarito baja um i, have a bachelor's b bachelor's in communication studies because it is a open field just does not deal with um, communication interactively but performing arts telecommunications business entrepreneurship so no i am not i am actually u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>301</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>[yes i am alright los angeles california yep um the lights big city it is always something going on the, traffic and that is it uh business administration and business management uh not that i am doing something else i, am doing networking at the moment sigh hmm uh to open up a big clothing line and just supply the, whole world with clothing laughter yeah definitely uh 'cause i am all about myself it is all about me laughter, not often but more times than not yeah being lied to and i guess when people think you are dumber, than you than what you are laughter uh try to get away from everything go into my own space very, well i am great at controlling my temper sigh i um probably like two days ago laughter and it was, probably just over some sports it was not a real argument sharp inhali...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>302</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[yeah that is perfectly fine i am feeling great uh it is an early morning i do not have school, i am excited laughter uh born and raised in los angeles california mhm uh the fact that you can surf, snowboard and like do two drastically different things in one day uh coming from the neighborhoods i have lived in, uh tisk I would say the population of uh lower socio economic backgrounds it it is like a cesspool almost, for young african americans and it sucks uh not really i have been to new york connecticut mexico and texas, but pretty young hmm laughter uh mostly if i am traveling by plane just the sight and the air is, amazing it is remarkable uh the first time i went to new york i was about eleven years old and, i made sure i got the the window seat on the plane and just the feeling ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>303</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[mm maybe okay i am from california it is sun there is sunshine s sunshine weather mhm what do you, like about l_a laughter what do laughter um i like um sniff i just said i i like li like, the weather the traffic no i do not have the means i studied liberal art a art a i studied, um film because i live in los angeles not really um uh to work i i want to become a, laughter i do not care ca i do not know i i i i do not wanna tell you yeah, okay i am very outgoing because because are you outgoing laughter um sit down sleep um i think about it, and then i stop it is easy um i do not remember um for not changing the battery on my, phone no it was the battery on my car i did not change it and it did not start and, i procrastinated and so yeah yeah that is a bad word you cannot say that i no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>304</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[i am doing alright uh originally i am from california uh born in glendale i am not too happy with, it uh just unemployed at the moment but uh actively seeking uh uh doing what i am supposed to be, doing uh there are some uh some prospects there but hopefully i will learn something today yeah well i have, one it is a girlfriend so i consider her a roommate a lover type thing pardon me um we are, pretty close uh i met her last year and uh we have been through some troubling times but uh we, are starting to see a light at the end of the tunnel uh the troubling times uh i would let, me see uh well i had uh gotten a d_u_i last year and uh lost my job i am a, truck driver by trade so uh that kinda threw me way off uh having a fourteen hundred dollar a week, job uh and then going to nothing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>488</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[synch laughter yes i am doing well i was born in oakland um sigh all the different areas of the, city uh the different cultures that are here the traffic the um lack of public transportation and the superficiality laughter, i i do not i would love to travel more um sigh being able to experience a different way of, living um getting outside the bubble that you normally live in and getting a different perspective um in two thousand, eight i studied abroad in brazil and that was about four months and i was in bahia which is in, the north and um let us see the program consisted of learning the history learning the language and staying with, a homestay family um i have always wanted to travel and i chose brazil because um they have an afrobrazilian, population or a large afrobrazilian popu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>489</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[synch yes huh overwhelmed i have a funeral to attend tomorrow i found out from my doctor i got some, health issues it is hard honey i am just putting one foot in front of the other and just trying, to get it done like i said i am overwhelmed but i cannot stop doing what i need to do, i am from united states of america i was born here in los angeles oh i love the beach honey, i like the weather um the beach and the weather oh the traffic and the people and this hollywood type, image uh not as much as i used to but i have been to europe i went on a uh, on a cruise um several cruise uh been to jamaica canada mexico spain morocco um london portugal por portugal yeah, and i am going to hawaii in august meeting new people and just seeing new things and just assimilating into, other people's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>490</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[yeah i am okay fine pensacola florida uh i met my wife in mississippi and she stays out here so, i moved to l_a what two thousand four about yeah about sixteen years twelve years ago something like that it, was pretty hard you know different environment i had to learn different things so the weather and there is a, lot to do uh too many people laughter uh i guess whenever i can uh two years ago uh we, are pretty close nah not at all i just do not really have the guess the time and the cash, mm not nothing not really nothing really makes me that mad anymore yeah i am very good at controlling my, temper mm not really sure when was the last time I would say maybe i am not not really sure, but it was just just about money m you know money uh i read the bible uh not really i, really do not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>491</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[yes alright i was born in uh los angeles california uh well the weather and uh the fact that uh, people are not so obtrusive like on the east coast uh just the uh the traffic and the the smoggy, weather i used to i am unemployed now so i rarely go anywhere i have a b_s in uh business, with a uh computer uh option i thought that the only place i could get a job with a business, bu business degree was in accounting or in business computer methods so i took that i took business computer methods, option oh I would probably be like uh a professional sports player or an actor something something that you enjoy, doing i like to uh ride my bike i like to watch sporting events now i cannot afford to go, to them so i pretty much watch on t_v i like to travel but like i say that i do, not have m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>492</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[yes i am doing pretty good laughter i am from oklahoma mhm yup oklahoma city uh the weather is great, i love being near the ocean and the palm trees and uh mountains i just love it here laughter um, it was pretty easy there is a few differences it is a huge city compared to where i am from, so traffic has definitely been an adjustment for me laughter um i just always wanted to live in california uh, hated being landlocked in oklahoma just wanted to get out to the west coast be near the ocean i studied, psychology and minored in art um no not at the moment um well actually i think i misunderstood your question, i am not still working on my degree but i am working in the field of psychology i well i, just got hired working at a group home for teenage boys who have been in trouble with t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>189 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Participant_ID  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text\n",
              "0               300  ...  [yes i am doing well los angeles the greater los angeles area people diversity and various entertainment and activities um, fickle weather traffic and litter i have traveled domestically not internationally seeing other places and how people live and the, culture the last trip that i went to was san diego but uh from childhood or as an adult um, family vacations to um baja and rosarito beach yeah the reason why that memory comes to mind because um this, weekend is easter and we usually will go down to easter east easter spring break to rosarito baja um i, have a bachelor's b bachelor's in communication studies because it is a open field just does not deal with um, communication interactively but performing arts telecommunications business entrepreneurship so no i am not i am actually u...\n",
              "1               301  ...  [yes i am alright los angeles california yep um the lights big city it is always something going on the, traffic and that is it uh business administration and business management uh not that i am doing something else i, am doing networking at the moment sigh hmm uh to open up a big clothing line and just supply the, whole world with clothing laughter yeah definitely uh 'cause i am all about myself it is all about me laughter, not often but more times than not yeah being lied to and i guess when people think you are dumber, than you than what you are laughter uh try to get away from everything go into my own space very, well i am great at controlling my temper sigh i um probably like two days ago laughter and it was, probably just over some sports it was not a real argument sharp inhali...\n",
              "0               302  ...  [yeah that is perfectly fine i am feeling great uh it is an early morning i do not have school, i am excited laughter uh born and raised in los angeles california mhm uh the fact that you can surf, snowboard and like do two drastically different things in one day uh coming from the neighborhoods i have lived in, uh tisk I would say the population of uh lower socio economic backgrounds it it is like a cesspool almost, for young african americans and it sucks uh not really i have been to new york connecticut mexico and texas, but pretty young hmm laughter uh mostly if i am traveling by plane just the sight and the air is, amazing it is remarkable uh the first time i went to new york i was about eleven years old and, i made sure i got the the window seat on the plane and just the feeling ...\n",
              "0               303  ...  [mm maybe okay i am from california it is sun there is sunshine s sunshine weather mhm what do you, like about l_a laughter what do laughter um i like um sniff i just said i i like li like, the weather the traffic no i do not have the means i studied liberal art a art a i studied, um film because i live in los angeles not really um uh to work i i want to become a, laughter i do not care ca i do not know i i i i do not wanna tell you yeah, okay i am very outgoing because because are you outgoing laughter um sit down sleep um i think about it, and then i stop it is easy um i do not remember um for not changing the battery on my, phone no it was the battery on my car i did not change it and it did not start and, i procrastinated and so yeah yeah that is a bad word you cannot say that i no...\n",
              "1               304  ...  [i am doing alright uh originally i am from california uh born in glendale i am not too happy with, it uh just unemployed at the moment but uh actively seeking uh uh doing what i am supposed to be, doing uh there are some uh some prospects there but hopefully i will learn something today yeah well i have, one it is a girlfriend so i consider her a roommate a lover type thing pardon me um we are, pretty close uh i met her last year and uh we have been through some troubling times but uh we, are starting to see a light at the end of the tunnel uh the troubling times uh i would let, me see uh well i had uh gotten a d_u_i last year and uh lost my job i am a, truck driver by trade so uh that kinda threw me way off uh having a fourteen hundred dollar a week, job uh and then going to nothing ...\n",
              "..              ...  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ...\n",
              "105             488  ...  [synch laughter yes i am doing well i was born in oakland um sigh all the different areas of the, city uh the different cultures that are here the traffic the um lack of public transportation and the superficiality laughter, i i do not i would love to travel more um sigh being able to experience a different way of, living um getting outside the bubble that you normally live in and getting a different perspective um in two thousand, eight i studied abroad in brazil and that was about four months and i was in bahia which is in, the north and um let us see the program consisted of learning the history learning the language and staying with, a homestay family um i have always wanted to travel and i chose brazil because um they have an afrobrazilian, population or a large afrobrazilian popu...\n",
              "32              489  ...  [synch yes huh overwhelmed i have a funeral to attend tomorrow i found out from my doctor i got some, health issues it is hard honey i am just putting one foot in front of the other and just trying, to get it done like i said i am overwhelmed but i cannot stop doing what i need to do, i am from united states of america i was born here in los angeles oh i love the beach honey, i like the weather um the beach and the weather oh the traffic and the people and this hollywood type, image uh not as much as i used to but i have been to europe i went on a uh, on a cruise um several cruise uh been to jamaica canada mexico spain morocco um london portugal por portugal yeah, and i am going to hawaii in august meeting new people and just seeing new things and just assimilating into, other people's...\n",
              "33              490  ...  [yeah i am okay fine pensacola florida uh i met my wife in mississippi and she stays out here so, i moved to l_a what two thousand four about yeah about sixteen years twelve years ago something like that it, was pretty hard you know different environment i had to learn different things so the weather and there is a, lot to do uh too many people laughter uh i guess whenever i can uh two years ago uh we, are pretty close nah not at all i just do not really have the guess the time and the cash, mm not nothing not really nothing really makes me that mad anymore yeah i am very good at controlling my, temper mm not really sure when was the last time I would say maybe i am not not really sure, but it was just just about money m you know money uh i read the bible uh not really i, really do not...\n",
              "106             491  ...  [yes alright i was born in uh los angeles california uh well the weather and uh the fact that uh, people are not so obtrusive like on the east coast uh just the uh the traffic and the the smoggy, weather i used to i am unemployed now so i rarely go anywhere i have a b_s in uh business, with a uh computer uh option i thought that the only place i could get a job with a business, bu business degree was in accounting or in business computer methods so i took that i took business computer methods, option oh I would probably be like uh a professional sports player or an actor something something that you enjoy, doing i like to uh ride my bike i like to watch sporting events now i cannot afford to go, to them so i pretty much watch on t_v i like to travel but like i say that i do, not have m...\n",
              "34              492  ...  [yes i am doing pretty good laughter i am from oklahoma mhm yup oklahoma city uh the weather is great, i love being near the ocean and the palm trees and uh mountains i just love it here laughter um, it was pretty easy there is a few differences it is a huge city compared to where i am from, so traffic has definitely been an adjustment for me laughter um i just always wanted to live in california uh, hated being landlocked in oklahoma just wanted to get out to the west coast be near the ocean i studied, psychology and minored in art um no not at the moment um well actually i think i misunderstood your question, i am not still working on my degree but i am working in the field of psychology i well i, just got hired working at a group home for teenage boys who have been in trouble with t...\n",
              "\n",
              "[189 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me5j1aGoutja"
      },
      "source": [
        "df_train = pd.DataFrame()\n",
        "df_val = pd.DataFrame()\n",
        "df_test = pd.DataFrame()\n",
        "\n",
        "df_train = final_df[final_df['split'] == 0]\n",
        "df_val = final_df[final_df['split'] == 1]\n",
        "df_test = final_df[final_df['split'] == 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tid5kCV-0yi1"
      },
      "source": [
        "#Copy dataframe to new dataframe\n",
        "df_train_over = df_train.copy()\n",
        "\n",
        "#Sample random rows\n",
        "n = 77 - 30 \n",
        "\n",
        "#Minority class\n",
        "minor = df_train[df_train['PHQ8_Binary'] == 1]\n",
        "\n",
        "rows = minor.sample(47, replace = True)\n",
        "\n",
        "over_train_df = df_train_over.append(rows, ignore_index=True)\n",
        "df_train = over_train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "AXBFpm2NuhBJ",
        "outputId": "6bbbde12-9d3f-49bb-a98f-54657d565e4c"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_plot_count(df_train, df_val, df_test): \n",
        "  not_depressed = []\n",
        "  depressed = []\n",
        "\n",
        "  for i in (df_train, df_val, df_test):\n",
        "    not_depressed.append(i.PHQ8_Binary.value_counts()[0])\n",
        "    depressed.append(i.PHQ8_Binary.value_counts()[1])\n",
        "\n",
        "  # bar charts\n",
        "\n",
        "  n_groups = 3\n",
        "\n",
        "  index = np.arange(n_groups)\n",
        "  bar_width = 0.35\n",
        "\n",
        "  rects1 = plt.bar(index, not_depressed, bar_width,\n",
        "                  color='b',\n",
        "                  label='Not depressed')\n",
        "\n",
        "  rects2 = plt.bar(index + bar_width, depressed, bar_width,\n",
        "                  color='r',\n",
        "                  label='Depressed')\n",
        "\n",
        "  plt.xlabel('Group')\n",
        "  plt.ylabel('Count')\n",
        "  plt.title('Depressed/Not depressed')\n",
        "  plt.xticks(index + bar_width / 2, ('train', 'val', 'test'))\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "  return depressed\n",
        "\n",
        "depressed = get_plot_count(df_train, df_val, df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN4yRf94uj6G"
      },
      "source": [
        "\"\"\"#If I wanted to downsample to 33, 33. \n",
        "#Rather upsample > more inputs.\n",
        "\n",
        "def downsample_df(df_train, i): \n",
        "  df_train_a = df_train[df_train['PHQ8_Binary'] == 0]\n",
        "  df_train_b = df_train[df_train['PHQ8_Binary'] == 1]\n",
        "\n",
        "  df_train_a = df_train_a.drop(df_train_a.index[depressed[i]:])\n",
        "  df_train = pd.concat([df_train_a, df_train_b])\n",
        "  return df_train\n",
        "\n",
        "df_train = downsample_df(df_train, 0)\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "szPPpQgmvBwX",
        "outputId": "b10c2da5-1c89-4be3-b439-f82e8180a33a"
      },
      "source": [
        "get_plot_count(df_train, df_val, df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[77, 12, 14]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "obg9qh1H5HDH",
        "outputId": "ba377f88-d0f8-4a3c-b725-f414e344da84"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Participant_ID</th>\n",
              "      <th>PHQ8_Binary</th>\n",
              "      <th>split</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>303</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[mm maybe okay california sun sunshine sunshine weather mhm like l_a laughter laughter um like um sniff said like li like weather traffic mean studied liberal art art studied um film live los angeles really um uh work want become laughter care ca know wanna tell yeah okay outgoing outgoing laughter um sit sleep um think stop easy um remember um changing battery phone battery car change start procrastinated yeah yeah bad word cannot say procrastinate something worse happen something worse happens something bad happens get worse become prepare preparation uh um prepare um think stuff last minute uh thing get thing done sigh context okay name laughter oh forgot okay um understand question elaborate give example sigh um um good question um ask question um okay sleep easy close eye tired ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>304</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[alright uh originally california uh born glendale happy uh unemployed moment uh actively seeking uh uh supposed uh uh prospect hopefully learn something today yeah well one girlfriend consider roommate lover type thing pardon um pretty close uh met last year uh troubling time uh starting see light end tunnel uh troubling time uh would let see uh well uh gotten d_u_i last year uh lost job truck driver trade uh kinda threw way uh fourteen hundred dollar week job uh going nothing kinda like plus losing license kinda really threw uh could um really uh found anything interested uh started cooking trying see route go far uh kind career uh choose choosing go culinary field know uh meanwhile still work hard find work uh without going school uh think today uh applied ralphs hopefully get job j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>305</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[yeah totally fine confidential know word wo word everybody's get wild laughter pretty good hanging nice weather um recent change feeling looking laughter digitial saying uh know yeah funny um tisk always thought uh conveying emotion feeling uh like better understood human interaction opposed non human entity yeah think see mean laughter um born raised los angeles venice area mar vista specifically um um well lot like like public transportation coming uh idea certain know medical drug like marijuana prevalent know topic thought upon feel los angeles' deep culture um stay pursue art lot lot gain different community understand laughter yeah um well human mentality um hectic chaotic know lot due vehicle car freeway idea grind know yeah hectic um atmosphere human atmosphere conducive suppo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>310</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[synch yes pretty good alexandria virginia mhm uh moved l_a uh end two thousand one yeah crazy laughter um couple time year usually like holiday hmm sigh sigh different um think lot mellow um way like suburbia nice um something comforting always like get little restless look forward coming back little action sniffle went um grad school u_s_c um bad mean program intense like first year consuming really much spare time think adjustment like laughter life made kind um easier guess also like community people went school automatically knew people know made friend stuff bad film hmm sigh uh epiphany one day um undergrad kinda floundering little bit really know wanted um one day kinda like know really excited really passionate um realized movie movie thing love passionate decided go film scho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>312</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[yes fine indiana yes uh know maybe ten year ago never uh like cold weather l_a beautiful weather activity time new life uh reasonably easy good weather ocean always something hm uh traffic sometimes seeing new place uh visiting new people well couple year ago went catalina island birthday spent whole day uh catalina island took boat uh hopping airplane leaving indiana good okay sleeping uh filmmaker one job yet hard shy hm past um trouble making new friend let l trusting people okay uh watch t_v go movie really temper sixteen stepfather uh uh nightmare nightmare childhood involving biological father far idea uh go walk drive uh maybe day ago uh walked around block time pushing people away uh guess try trust little bit let life little bit well that'd easy that'd first half life it'd ea...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Participant_ID  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text\n",
              "0             303  ...  [mm maybe okay california sun sunshine sunshine weather mhm like l_a laughter laughter um like um sniff said like li like weather traffic mean studied liberal art art studied um film live los angeles really um uh work want become laughter care ca know wanna tell yeah okay outgoing outgoing laughter um sit sleep um think stop easy um remember um changing battery phone battery car change start procrastinated yeah yeah bad word cannot say procrastinate something worse happen something worse happens something bad happens get worse become prepare preparation uh um prepare um think stuff last minute uh thing get thing done sigh context okay name laughter oh forgot okay um understand question elaborate give example sigh um um good question um ask question um okay sleep easy close eye tired ti...\n",
              "1             304  ...  [alright uh originally california uh born glendale happy uh unemployed moment uh actively seeking uh uh supposed uh uh prospect hopefully learn something today yeah well one girlfriend consider roommate lover type thing pardon um pretty close uh met last year uh troubling time uh starting see light end tunnel uh troubling time uh would let see uh well uh gotten d_u_i last year uh lost job truck driver trade uh kinda threw way uh fourteen hundred dollar week job uh going nothing kinda like plus losing license kinda really threw uh could um really uh found anything interested uh started cooking trying see route go far uh kind career uh choose choosing go culinary field know uh meanwhile still work hard find work uh without going school uh think today uh applied ralphs hopefully get job j...\n",
              "2             305  ...  [yeah totally fine confidential know word wo word everybody's get wild laughter pretty good hanging nice weather um recent change feeling looking laughter digitial saying uh know yeah funny um tisk always thought uh conveying emotion feeling uh like better understood human interaction opposed non human entity yeah think see mean laughter um born raised los angeles venice area mar vista specifically um um well lot like like public transportation coming uh idea certain know medical drug like marijuana prevalent know topic thought upon feel los angeles' deep culture um stay pursue art lot lot gain different community understand laughter yeah um well human mentality um hectic chaotic know lot due vehicle car freeway idea grind know yeah hectic um atmosphere human atmosphere conducive suppo...\n",
              "3             310  ...  [synch yes pretty good alexandria virginia mhm uh moved l_a uh end two thousand one yeah crazy laughter um couple time year usually like holiday hmm sigh sigh different um think lot mellow um way like suburbia nice um something comforting always like get little restless look forward coming back little action sniffle went um grad school u_s_c um bad mean program intense like first year consuming really much spare time think adjustment like laughter life made kind um easier guess also like community people went school automatically knew people know made friend stuff bad film hmm sigh uh epiphany one day um undergrad kinda floundering little bit really know wanted um one day kinda like know really excited really passionate um realized movie movie thing love passionate decided go film scho...\n",
              "4             312  ...  [yes fine indiana yes uh know maybe ten year ago never uh like cold weather l_a beautiful weather activity time new life uh reasonably easy good weather ocean always something hm uh traffic sometimes seeing new place uh visiting new people well couple year ago went catalina island birthday spent whole day uh catalina island took boat uh hopping airplane leaving indiana good okay sleeping uh filmmaker one job yet hard shy hm past um trouble making new friend let l trusting people okay uh watch t_v go movie really temper sixteen stepfather uh uh nightmare nightmare childhood involving biological father far idea uh go walk drive uh maybe day ago uh walked around block time pushing people away uh guess try trust little bit let life little bit well that'd easy that'd first half life it'd ea...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3NDjnW8j4Ef"
      },
      "source": [
        "os.chdir('/content/drive/My Drive')\n",
        "df_train.to_csv(\"df_train.csv\", index=False)\n",
        "df_val.to_csv(\"df_val.csv\", index=False)\n",
        "df_test.to_csv(\"df_test.csv\", index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "F2-Mevyo4l0d",
        "outputId": "10cc4c4b-4caf-47fb-8148-8162b474f94d"
      },
      "source": [
        "test_df = pd.read_csv(\"df_train.csv\")\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Participant_ID</th>\n",
              "      <th>PHQ8_Binary</th>\n",
              "      <th>split</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>303</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[\"mm maybe okay california sun sunshine sunshine weather mhm like l_a laughter laughter um like um sniff said like li like weather traffic mean studied liberal art art studied um film live los angeles really um uh work want become laughter care ca know wanna tell yeah okay outgoing outgoing laughter um sit sleep um think stop easy um remember um changing battery phone battery car change start procrastinated yeah yeah bad word cannot say procrastinate something worse happen something worse happens something bad happens get worse become prepare preparation uh um prepare um think stuff last minute uh thing get thing done sigh context okay name laughter oh forgot okay um understand question elaborate give example sigh um um good question um ask question um okay sleep easy close eye tired t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>304</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>['alright uh originally california uh born glendale happy uh unemployed moment uh actively seeking uh uh supposed uh uh prospect hopefully learn something today yeah well one girlfriend consider roommate lover type thing pardon um pretty close uh met last year uh troubling time uh starting see light end tunnel uh troubling time uh would let see uh well uh gotten d_u_i last year uh lost job truck driver trade uh kinda threw way uh fourteen hundred dollar week job uh going nothing kinda like plus losing license kinda really threw uh could um really uh found anything interested uh started cooking trying see route go far uh kind career uh choose choosing go culinary field know uh meanwhile still work hard find work uh without going school uh think today uh applied ralphs hopefully get job ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>305</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[\"yeah totally fine confidential know word wo word everybody's get wild laughter pretty good hanging nice weather um recent change feeling looking laughter digitial saying uh know yeah funny um tisk always thought uh conveying emotion feeling uh like better understood human interaction opposed non human entity yeah think see mean laughter um born raised los angeles venice area mar vista specifically um um well lot like like public transportation coming uh idea certain know medical drug like marijuana prevalent know topic thought upon feel los angeles' deep culture um stay pursue art lot lot gain different community understand laughter yeah um well human mentality um hectic chaotic know lot due vehicle car freeway idea grind know yeah hectic um atmosphere human atmosphere conducive supp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>310</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[\"synch yes pretty good alexandria virginia mhm uh moved l_a uh end two thousand one yeah crazy laughter um couple time year usually like holiday hmm sigh sigh different um think lot mellow um way like suburbia nice um something comforting always like get little restless look forward coming back little action sniffle went um grad school u_s_c um bad mean program intense like first year consuming really much spare time think adjustment like laughter life made kind um easier guess also like community people went school automatically knew people know made friend stuff bad film hmm sigh uh epiphany one day um undergrad kinda floundering little bit really know wanted um one day kinda like know really excited really passionate um realized movie movie thing love passionate decided go film sch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>312</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[\"yes fine indiana yes uh know maybe ten year ago never uh like cold weather l_a beautiful weather activity time new life uh reasonably easy good weather ocean always something hm uh traffic sometimes seeing new place uh visiting new people well couple year ago went catalina island birthday spent whole day uh catalina island took boat uh hopping airplane leaving indiana good okay sleeping uh filmmaker one job yet hard shy hm past um trouble making new friend let l trusting people okay uh watch t_v go movie really temper sixteen stepfather uh uh nightmare nightmare childhood involving biological father far idea uh go walk drive uh maybe day ago uh walked around block time pushing people away uh guess try trust little bit let life little bit well that'd easy that'd first half life it'd e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Participant_ID  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text\n",
              "0             303  ...  [\"mm maybe okay california sun sunshine sunshine weather mhm like l_a laughter laughter um like um sniff said like li like weather traffic mean studied liberal art art studied um film live los angeles really um uh work want become laughter care ca know wanna tell yeah okay outgoing outgoing laughter um sit sleep um think stop easy um remember um changing battery phone battery car change start procrastinated yeah yeah bad word cannot say procrastinate something worse happen something worse happens something bad happens get worse become prepare preparation uh um prepare um think stuff last minute uh thing get thing done sigh context okay name laughter oh forgot okay um understand question elaborate give example sigh um um good question um ask question um okay sleep easy close eye tired t...\n",
              "1             304  ...  ['alright uh originally california uh born glendale happy uh unemployed moment uh actively seeking uh uh supposed uh uh prospect hopefully learn something today yeah well one girlfriend consider roommate lover type thing pardon um pretty close uh met last year uh troubling time uh starting see light end tunnel uh troubling time uh would let see uh well uh gotten d_u_i last year uh lost job truck driver trade uh kinda threw way uh fourteen hundred dollar week job uh going nothing kinda like plus losing license kinda really threw uh could um really uh found anything interested uh started cooking trying see route go far uh kind career uh choose choosing go culinary field know uh meanwhile still work hard find work uh without going school uh think today uh applied ralphs hopefully get job ...\n",
              "2             305  ...  [\"yeah totally fine confidential know word wo word everybody's get wild laughter pretty good hanging nice weather um recent change feeling looking laughter digitial saying uh know yeah funny um tisk always thought uh conveying emotion feeling uh like better understood human interaction opposed non human entity yeah think see mean laughter um born raised los angeles venice area mar vista specifically um um well lot like like public transportation coming uh idea certain know medical drug like marijuana prevalent know topic thought upon feel los angeles' deep culture um stay pursue art lot lot gain different community understand laughter yeah um well human mentality um hectic chaotic know lot due vehicle car freeway idea grind know yeah hectic um atmosphere human atmosphere conducive supp...\n",
              "3             310  ...  [\"synch yes pretty good alexandria virginia mhm uh moved l_a uh end two thousand one yeah crazy laughter um couple time year usually like holiday hmm sigh sigh different um think lot mellow um way like suburbia nice um something comforting always like get little restless look forward coming back little action sniffle went um grad school u_s_c um bad mean program intense like first year consuming really much spare time think adjustment like laughter life made kind um easier guess also like community people went school automatically knew people know made friend stuff bad film hmm sigh uh epiphany one day um undergrad kinda floundering little bit really know wanted um one day kinda like know really excited really passionate um realized movie movie thing love passionate decided go film sch...\n",
              "4             312  ...  [\"yes fine indiana yes uh know maybe ten year ago never uh like cold weather l_a beautiful weather activity time new life uh reasonably easy good weather ocean always something hm uh traffic sometimes seeing new place uh visiting new people well couple year ago went catalina island birthday spent whole day uh catalina island took boat uh hopping airplane leaving indiana good okay sleeping uh filmmaker one job yet hard shy hm past um trouble making new friend let l trusting people okay uh watch t_v go movie really temper sixteen stepfather uh uh nightmare nightmare childhood involving biological father far idea uh go walk drive uh maybe day ago uh walked around block time pushing people away uh guess try trust little bit let life little bit well that'd easy that'd first half life it'd e...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9pLYq7Sm-aM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "e70e6c34-265c-4152-e757-a6fad5527415"
      },
      "source": [
        "##How to check length of all inputs. \n",
        "seq_len = [len(i.split()) for i in df_train.text[3]]\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc6179db410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX74KAyPka44"
      },
      "source": [
        "def prepare_reduced_data(data):\n",
        "    df = data\n",
        "    df = df.set_index(pd.Series(list(range(108))))\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otsCognjsSSE"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFalfJhi5mX0",
        "outputId": "5c8e6dd7-dccd-4b39-897c-08e32dfcd550"
      },
      "source": [
        "class_count_df = df_train.groupby('PHQ8_Binary').count() \n",
        "n_0, n_1 = class_count_df.iloc[0, 0], class_count_df.iloc[1, 0] \n",
        "w_0 = (n_0 + n_1) / (2.0 * n_0)\n",
        "w_1 = (n_0 + n_1) / (2.0 * n_1)\n",
        "print(w_0, w_1) \n",
        "class_weights=torch.FloatTensor([w_0, w_1]).cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkGONjp0kMZG"
      },
      "source": [
        "def defining_bert_tokenizer(PRE_TRAINED_MODEL_NAME):\n",
        "    tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, truncation=True)\n",
        "    return tokenizer\n",
        "\n",
        "class GPReviewDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, doc, targets, tokenizer, max_len):\n",
        "        self.doc = doc\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.doc)\n",
        "  \n",
        "    def __getitem__(self, item):\n",
        "        doc = str(self.doc[item])\n",
        "        target = self.targets[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "                    doc,\n",
        "                    add_special_tokens=True,\n",
        "                    max_length=self.max_len,\n",
        "                    return_token_type_ids=False,\n",
        "                    pad_to_max_length=True,\n",
        "                    return_attention_mask=True,\n",
        "                    return_tensors='pt',\n",
        "                    )\n",
        "\n",
        "        return {\n",
        "            'doc_text': doc,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'targets': torch.tensor(target, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "    ds = GPReviewDataset(\n",
        "        doc=df.text.to_numpy(),\n",
        "        targets=df.PHQ8_Binary.to_numpy(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "      )\n",
        "    \n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=2\n",
        "        )\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "        self.drop = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, pooled_output = self.bert(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask\n",
        "        )\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)\n",
        "\n",
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "    ):\n",
        "    model = model.train()\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        targets = d[\"targets\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == targets)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "    model = model.eval()\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            targets = d[\"targets\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "            )\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == targets)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def get_predictions(model, data_loader):\n",
        "    model = model.eval()\n",
        "\n",
        "    review_texts = []\n",
        "    predictions = []\n",
        "    prediction_probs = []\n",
        "    real_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "\n",
        "            texts = d[\"doc_text\"]\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            targets = d[\"targets\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "            )\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "            review_texts.extend(texts)\n",
        "            predictions.extend(preds)\n",
        "            prediction_probs.extend(probs)\n",
        "            real_values.extend(targets)\n",
        "\n",
        "    predictions = torch.stack(predictions).cpu()\n",
        "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "    real_values = torch.stack(real_values).cpu()\n",
        "    return review_texts, predictions, prediction_probs, real_values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlJ7cg5laeDy",
        "outputId": "cd74f2d0-ff05-4229-83e7-85bedceddb30"
      },
      "source": [
        "if __name__==\"__main__\":\n",
        "    \n",
        "    \"\"\"full_data=final_df\n",
        "    df=prepare_reduced_data(full_data)\"\"\"\n",
        "\n",
        "    PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "    tokenizer=defining_bert_tokenizer(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "    sample_txt=sample_txt='This is the sample text I will be using/Lets go' \n",
        "\n",
        "    tokens = tokenizer.tokenize(sample_txt)\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    encoding = tokenizer.encode_plus(\n",
        "                      sample_txt,\n",
        "                      max_length=20,\n",
        "                      #64\n",
        "                      add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "                      return_token_type_ids=False,\n",
        "                      pad_to_max_length=True,\n",
        "                      trunctation=True,\n",
        "                      return_attention_mask=True,\n",
        "                      return_tensors='pt',  # Return PyTorch tensors\n",
        "                    )\n",
        "\n",
        "    \n",
        "    \n",
        "    MAX_LEN = 20\n",
        "    #256\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "    val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "    test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "    data = next(iter(train_data_loader))\n",
        "\n",
        "    bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "    last_hidden_state, pooled_output = bert_model(\n",
        "                    input_ids=encoding['input_ids'], \n",
        "                    attention_mask=encoding['attention_mask'])\n",
        "\n",
        "    model = SentimentClassifier(2)\n",
        "    model = model.to(device)\n",
        "\n",
        "    input_ids = data['input_ids'].to(device)\n",
        "    attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "\n",
        "    EPOCHS = 10\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=0.00001, correct_bias=False)\n",
        "    total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(weight = class_weights).to(device)\n",
        "\n",
        "\n",
        "    history = defaultdict(list)\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_data_loader,    \n",
        "        loss_fn, \n",
        "        optimizer, \n",
        "        device, \n",
        "        scheduler, \n",
        "        len(df_train)\n",
        "        )\n",
        "\n",
        "        print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "        val_acc, val_loss = eval_model(\n",
        "        model,\n",
        "        val_data_loader,\n",
        "        loss_fn, \n",
        "        device, \n",
        "        len(df_val)\n",
        "        )\n",
        "\n",
        "        print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "        print()\n",
        "\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        \"\"\"if val_acc > best_accuracy:\n",
        "          torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "          best_accuracy = val_acc\"\"\"\n",
        "\n",
        "        y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "        model,\n",
        "        test_data_loader\n",
        "        )\n",
        "\n",
        "        print(classification_report(y_test, y_pred, target_names=['Not depressed', 'Depressed']))\n",
        "        \n",
        "        report = classification_report(y_test, y_pred, target_names=['Not depressed', 'Depressed'], output_dict=True)\n",
        "\n",
        "        weighted_f1 = report['weighted avg']['f1-score']\n",
        "\n",
        "        if weighted_f1 > best_accuracy:\n",
        "            torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "            best_accuracy = weighted_f1\n",
        "        \n",
        "    test_acc, _ = eval_model(\n",
        "          model,\n",
        "          test_data_loader,\n",
        "          loss_fn,\n",
        "          device,\n",
        "          len(df_test)\n",
        "        )\n",
        "    print('\\nTest Accuracy:\\n')\n",
        "    print(test_acc.item())\n",
        "\n",
        "    y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "    model,\n",
        "    test_data_loader\n",
        "    )\n",
        "\n",
        "    print(classification_report(y_test, y_pred, target_names=['Not depressed', 'Depressed']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "----------\n",
            "Train loss 0.8752265334129333 accuracy 0.38961038961038963\n",
            "Val   loss 0.6252734065055847 accuracy 0.6571428571428571\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.70      1.00      0.82        33\n",
            "    Depressed       0.00      0.00      0.00        14\n",
            "\n",
            "     accuracy                           0.70        47\n",
            "    macro avg       0.35      0.50      0.41        47\n",
            " weighted avg       0.49      0.70      0.58        47\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train loss 0.7976791381835937 accuracy 0.4415584415584416\n",
            "Val   loss 0.5815731734037399 accuracy 0.6571428571428571\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.70      1.00      0.82        33\n",
            "    Depressed       0.00      0.00      0.00        14\n",
            "\n",
            "     accuracy                           0.70        47\n",
            "    macro avg       0.35      0.50      0.41        47\n",
            " weighted avg       0.49      0.70      0.58        47\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train loss 0.7372286081314087 accuracy 0.4675324675324676\n",
            "Val   loss 0.5575634092092514 accuracy 0.6571428571428571\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.70      1.00      0.82        33\n",
            "    Depressed       0.00      0.00      0.00        14\n",
            "\n",
            "     accuracy                           0.70        47\n",
            "    macro avg       0.35      0.50      0.41        47\n",
            " weighted avg       0.49      0.70      0.58        47\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train loss 0.746341609954834 accuracy 0.538961038961039\n",
            "Val   loss 0.5248341411352158 accuracy 0.6571428571428571\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.70      1.00      0.82        33\n",
            "    Depressed       0.00      0.00      0.00        14\n",
            "\n",
            "     accuracy                           0.70        47\n",
            "    macro avg       0.35      0.50      0.41        47\n",
            " weighted avg       0.49      0.70      0.58        47\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train loss 0.6773711800575256 accuracy 0.6038961038961039\n",
            "Val   loss 0.5528568774461746 accuracy 0.6285714285714286\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.70      0.91      0.79        33\n",
            "    Depressed       0.25      0.07      0.11        14\n",
            "\n",
            "     accuracy                           0.66        47\n",
            "    macro avg       0.47      0.49      0.45        47\n",
            " weighted avg       0.56      0.66      0.59        47\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train loss 0.591442346572876 accuracy 0.6883116883116883\n",
            "Val   loss 0.5695814341306686 accuracy 0.4857142857142857\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.68      0.82      0.74        33\n",
            "    Depressed       0.14      0.07      0.10        14\n",
            "\n",
            "     accuracy                           0.60        47\n",
            "    macro avg       0.41      0.44      0.42        47\n",
            " weighted avg       0.52      0.60      0.55        47\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train loss 0.4907252252101898 accuracy 0.8051948051948052\n",
            "Val   loss 0.574759915471077 accuracy 0.4857142857142857\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.68      0.79      0.73        33\n",
            "    Depressed       0.22      0.14      0.17        14\n",
            "\n",
            "     accuracy                           0.60        47\n",
            "    macro avg       0.45      0.47      0.45        47\n",
            " weighted avg       0.55      0.60      0.57        47\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train loss 0.40686517357826235 accuracy 0.8831168831168832\n",
            "Val   loss 0.615808367729187 accuracy 0.4857142857142857\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.67      0.67      0.67        33\n",
            "    Depressed       0.21      0.21      0.21        14\n",
            "\n",
            "     accuracy                           0.53        47\n",
            "    macro avg       0.44      0.44      0.44        47\n",
            " weighted avg       0.53      0.53      0.53        47\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train loss 0.3353577971458435 accuracy 0.9415584415584416\n",
            "Val   loss 0.5875384211540222 accuracy 0.42857142857142855\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.69      0.73      0.71        33\n",
            "    Depressed       0.25      0.21      0.23        14\n",
            "\n",
            "     accuracy                           0.57        47\n",
            "    macro avg       0.47      0.47      0.47        47\n",
            " weighted avg       0.56      0.57      0.56        47\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train loss 0.32668676376342776 accuracy 0.9090909090909092\n",
            "Val   loss 0.5922346785664558 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.68      0.76      0.71        33\n",
            "    Depressed       0.20      0.14      0.17        14\n",
            "\n",
            "     accuracy                           0.57        47\n",
            "    macro avg       0.44      0.45      0.44        47\n",
            " weighted avg       0.53      0.57      0.55        47\n",
            "\n",
            "\n",
            "Test Accuracy:\n",
            "\n",
            "0.5744680851063829\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.68      0.76      0.71        33\n",
            "    Depressed       0.20      0.14      0.17        14\n",
            "\n",
            "     accuracy                           0.57        47\n",
            "    macro avg       0.44      0.45      0.44        47\n",
            " weighted avg       0.53      0.57      0.55        47\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G3P1yfoWtqf"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_accuracy = history.history['train_acc']\n",
        "val_accuracy = history.history['val_acc']\n",
        "train_loss = history.history['train_loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(18,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('Training and Validation accuracy by epoch', fontsize=16)\n",
        "plt.plot(train_accuracy, label='Training accuracy')\n",
        "plt.plot(val_accuracy, label='Validation accuracy')\n",
        "plt.legend(['Train acc', 'Val acc'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title('Training and Validation loss by epoch', fontsize=16)\n",
        "plt.plot(train_loss, label='Training loss')\n",
        "plt.plot(val_loss, label='Validation loss')\n",
        "plt.legend(['Train loss', 'Val loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOoyktDzpppb"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBup8NR89clQ",
        "outputId": "d7046055-84a6-459a-8eff-e2b44b5a066f"
      },
      "source": [
        "history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {'train_acc': [tensor(0.5325, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.4416, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.5195, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.5260, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.4805, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.5779, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6429, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.8052, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.9221, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.9675, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.9935, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(1., device='cuda:0', dtype=torch.float64),\n",
              "              tensor(1., device='cuda:0', dtype=torch.float64),\n",
              "              tensor(1., device='cuda:0', dtype=torch.float64),\n",
              "              tensor(1., device='cuda:0', dtype=torch.float64),\n",
              "              tensor(1., device='cuda:0', dtype=torch.float64),\n",
              "              tensor(1., device='cuda:0', dtype=torch.float64),\n",
              "              tensor(1., device='cuda:0', dtype=torch.float64),\n",
              "              tensor(1., device='cuda:0', dtype=torch.float64),\n",
              "              tensor(1., device='cuda:0', dtype=torch.float64)],\n",
              "             'train_loss': [0.7561469078063965,\n",
              "              0.6989412903785706,\n",
              "              0.6766046583652496,\n",
              "              0.6721520185470581,\n",
              "              0.6842444658279419,\n",
              "              0.6164949238300323,\n",
              "              0.5486943006515503,\n",
              "              0.39398004710674284,\n",
              "              0.2628019735217094,\n",
              "              0.1440986379981041,\n",
              "              0.072610305249691,\n",
              "              0.028231996670365335,\n",
              "              0.01627161502838135,\n",
              "              0.008600341528654099,\n",
              "              0.005948245199397207,\n",
              "              0.0049946046434342865,\n",
              "              0.00450070844963193,\n",
              "              0.003609176794998348,\n",
              "              0.003114050580188632,\n",
              "              0.0034807323012501002],\n",
              "             'val_acc': [tensor(0.3429, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.3429, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.3429, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.3429, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.3429, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.3429, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6571, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6857, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6286, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6571, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6857, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6857, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6286, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6857, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6857, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6571, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6286, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6286, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6286, device='cuda:0', dtype=torch.float64),\n",
              "              tensor(0.6286, device='cuda:0', dtype=torch.float64)],\n",
              "             'val_loss': [0.979504257440567,\n",
              "              0.7765416204929352,\n",
              "              0.8299268484115601,\n",
              "              0.8615255653858185,\n",
              "              0.8462762534618378,\n",
              "              0.9031248986721039,\n",
              "              0.6341874301433563,\n",
              "              0.6455429196357727,\n",
              "              0.5835268050432205,\n",
              "              0.6309498250484467,\n",
              "              0.7176379486918449,\n",
              "              0.856854896992445,\n",
              "              1.1059282533824444,\n",
              "              0.9817250333726406,\n",
              "              1.0194463655352592,\n",
              "              1.1347554940730333,\n",
              "              1.2593799168244004,\n",
              "              1.3179565966129303,\n",
              "              1.3371074760798365,\n",
              "              1.3384845650289208]})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAgMWWp9xwPg"
      },
      "source": [
        "From here on and further, we are using the fine-tuned model rather than the general BERT model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nIA0K_Hvhcb",
        "outputId": "8a05f609-3991-4d17-c67a-f5662134ce38"
      },
      "source": [
        "if __name__==\"__main__\":\n",
        "    \n",
        "    \"\"\"full_data=final_df\n",
        "    df=prepare_reduced_data(full_data)\"\"\"\n",
        "\n",
        "    PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "    tokenizer=defining_bert_tokenizer(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "    sample_txt='This is the sample text I will be using/Lets go' \n",
        "\n",
        "    tokens = tokenizer.tokenize(sample_txt)\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    encoding = tokenizer.encode_plus(\n",
        "                      sample_txt,\n",
        "                      max_length=200,\n",
        "                      #64\n",
        "                      add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "                      return_token_type_ids=False,\n",
        "                      pad_to_max_length=True,\n",
        "                      trunctation=True,\n",
        "                      return_attention_mask=True,\n",
        "                      return_tensors='pt',  # Return PyTorch tensors\n",
        "                    )\n",
        "\n",
        "    \n",
        "    \n",
        "    MAX_LEN = 200\n",
        "    #256\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "    val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "    test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "    data = next(iter(train_data_loader))\n",
        "\n",
        "    bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "    last_hidden_state, pooled_output = bert_model(\n",
        "                    input_ids=encoding['input_ids'], \n",
        "                    attention_mask=encoding['attention_mask'])\n",
        "\n",
        "    model = SentimentClassifier(2)\n",
        "\n",
        "    #Load fine-tuned weights to model.\n",
        "    os.chdir('/content/drive/My Drive')\n",
        "    path = 'saved_weights3_1.pt'\n",
        "    model.load_state_dict(torch.load(path))\n",
        "\n",
        "    #Send model to device (GPU)\n",
        "    model = model.to(device)\n",
        "\n",
        "    #Send input and attention mask to device (GPU)\n",
        "    input_ids = data['input_ids'].to(device)\n",
        "    attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "\n",
        "    EPOCHS = 20\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=0.00001, correct_bias=False)\n",
        "    total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(weight = class_weights).to(device)\n",
        "\n",
        "\n",
        "    history = defaultdict(list)\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_data_loader,    \n",
        "        loss_fn, \n",
        "        optimizer, \n",
        "        device, \n",
        "        scheduler, \n",
        "        len(df_train)\n",
        "        )\n",
        "\n",
        "        print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "        val_acc, val_loss = eval_model(\n",
        "        model,\n",
        "        val_data_loader,\n",
        "        loss_fn, \n",
        "        device, \n",
        "        len(df_val)\n",
        "        )\n",
        "\n",
        "        print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "        print()\n",
        "\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        \"\"\"if val_acc > best_accuracy:\n",
        "          torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "          best_accuracy = val_acc\"\"\"\n",
        "\n",
        "        y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "        model,\n",
        "        test_data_loader\n",
        "        )\n",
        "\n",
        "        print(classification_report(y_test, y_pred, target_names=['Not depressed', 'Depressed']))\n",
        "        \n",
        "        report = classification_report(y_test, y_pred, target_names=['Not depressed', 'Depressed'], output_dict=True)\n",
        "\n",
        "        weighted_f1 = report['weighted avg']['f1-score']\n",
        "\n",
        "        if weighted_f1 > best_accuracy:\n",
        "            torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "            best_accuracy = weighted_f1\n",
        "        \n",
        "    test_acc, _ = eval_model(\n",
        "          model,\n",
        "          test_data_loader,\n",
        "          loss_fn,\n",
        "          device,\n",
        "          len(df_test)\n",
        "        )\n",
        "    print('\\nTest Accuracy:\\n')\n",
        "    print(test_acc.item())\n",
        "\n",
        "    y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "    model,\n",
        "    test_data_loader\n",
        "    )\n",
        "\n",
        "    print(classification_report(y_test, y_pred, target_names=['Not depressed', 'Depressed']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "----------\n",
            "Train loss 1.1051594257354735 accuracy 0.35064935064935066\n",
            "Val   loss 0.6147240996360779 accuracy 0.6857142857142857\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.77      0.73      0.75        33\n",
            "    Depressed       0.44      0.50      0.47        14\n",
            "\n",
            "     accuracy                           0.66        47\n",
            "    macro avg       0.61      0.61      0.61        47\n",
            " weighted avg       0.67      0.66      0.67        47\n",
            "\n",
            "Epoch 2/20\n",
            "----------\n",
            "Train loss 0.5064523786306381 accuracy 0.6688311688311689\n",
            "Val   loss 0.8374199569225311 accuracy 0.39999999999999997\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.83      0.30      0.44        33\n",
            "    Depressed       0.34      0.86      0.49        14\n",
            "\n",
            "     accuracy                           0.47        47\n",
            "    macro avg       0.59      0.58      0.47        47\n",
            " weighted avg       0.69      0.47      0.46        47\n",
            "\n",
            "Epoch 3/20\n",
            "----------\n",
            "Train loss 0.2842897206544876 accuracy 0.8766233766233766\n",
            "Val   loss 0.7338202968239784 accuracy 0.6285714285714286\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.85      0.79        33\n",
            "    Depressed       0.44      0.29      0.35        14\n",
            "\n",
            "     accuracy                           0.68        47\n",
            "    macro avg       0.59      0.57      0.57        47\n",
            " weighted avg       0.65      0.68      0.66        47\n",
            "\n",
            "Epoch 4/20\n",
            "----------\n",
            "Train loss 0.04949127472937107 accuracy 0.9935064935064936\n",
            "Val   loss 1.2979472801089287 accuracy 0.4857142857142857\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.77      0.82      0.79        33\n",
            "    Depressed       0.50      0.43      0.46        14\n",
            "\n",
            "     accuracy                           0.70        47\n",
            "    macro avg       0.64      0.62      0.63        47\n",
            " weighted avg       0.69      0.70      0.70        47\n",
            "\n",
            "Epoch 5/20\n",
            "----------\n",
            "Train loss 0.005205160565674305 accuracy 1.0\n",
            "Val   loss 1.7653589034453034 accuracy 0.4857142857142857\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.75      0.73      0.74        33\n",
            "    Depressed       0.40      0.43      0.41        14\n",
            "\n",
            "     accuracy                           0.64        47\n",
            "    macro avg       0.57      0.58      0.58        47\n",
            " weighted avg       0.65      0.64      0.64        47\n",
            "\n",
            "Epoch 6/20\n",
            "----------\n",
            "Train loss 0.0019198768539354205 accuracy 1.0\n",
            "Val   loss 1.9671528991311789 accuracy 0.4857142857142857\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "Epoch 7/20\n",
            "----------\n",
            "Train loss 0.001546782860532403 accuracy 1.0\n",
            "Val   loss 2.061838166322559 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "Epoch 8/20\n",
            "----------\n",
            "Train loss 0.0011324111605063082 accuracy 1.0\n",
            "Val   loss 2.1220096461474895 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "Epoch 9/20\n",
            "----------\n",
            "Train loss 0.000956949417013675 accuracy 1.0\n",
            "Val   loss 2.1638860744424164 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "Epoch 10/20\n",
            "----------\n",
            "Train loss 0.000895428447984159 accuracy 1.0\n",
            "Val   loss 2.1999334767460823 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "Epoch 11/20\n",
            "----------\n",
            "Train loss 0.0007910603075288236 accuracy 1.0\n",
            "Val   loss 2.227528058923781 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "Epoch 12/20\n",
            "----------\n",
            "Train loss 0.0007711103418841958 accuracy 1.0\n",
            "Val   loss 2.242300079204142 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "Epoch 13/20\n",
            "----------\n",
            "Train loss 0.0006545018870383501 accuracy 1.0\n",
            "Val   loss 2.252814803039655 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "Epoch 14/20\n",
            "----------\n",
            "Train loss 0.0006525581469759345 accuracy 1.0\n",
            "Val   loss 2.2605238701216877 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "Epoch 15/20\n",
            "----------\n",
            "Train loss 0.0005673302803188562 accuracy 1.0\n",
            "Val   loss 2.2663543227827176 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "Epoch 16/20\n",
            "----------\n",
            "Train loss 0.0005642963107675314 accuracy 1.0\n",
            "Val   loss 2.270802933606319 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "Epoch 17/20\n",
            "----------\n",
            "Train loss 0.0005768210743553936 accuracy 1.0\n",
            "Val   loss 2.274064306053333 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "Epoch 18/20\n",
            "----------\n",
            "Train loss 0.0005530241644009948 accuracy 1.0\n",
            "Val   loss 2.2765413211891428 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "Epoch 19/20\n",
            "----------\n",
            "Train loss 0.0005995141109451652 accuracy 1.0\n",
            "Val   loss 2.2781330306315795 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "Epoch 20/20\n",
            "----------\n",
            "Train loss 0.0005323589895851911 accuracy 1.0\n",
            "Val   loss 2.278709880192764 accuracy 0.45714285714285713\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n",
            "\n",
            "Test Accuracy:\n",
            "\n",
            "0.6170212765957447\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not depressed       0.74      0.70      0.72        33\n",
            "    Depressed       0.38      0.43      0.40        14\n",
            "\n",
            "     accuracy                           0.62        47\n",
            "    macro avg       0.56      0.56      0.56        47\n",
            " weighted avg       0.63      0.62      0.62        47\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jth7a7Um1-hA",
        "outputId": "d6500068-4456-4263-e515-0ffa1fd379a5"
      },
      "source": [
        "os.chdir('/content/drive/My Drive')\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptZ3_Gky2D4z"
      },
      "source": [
        "os.chdir('/content/drive/My Drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p1Q55Q2NSsU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}